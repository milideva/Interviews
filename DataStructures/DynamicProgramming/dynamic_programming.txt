1.Define subproblems
2.Write down the recurrence that relates subproblems
3.Recognize and solve the base case

https://en.wikipedia.org/wiki/Dynamic_programming

There are two key attributes that a problem must have in order for dynamic
programming to be applicable: optimal substructure and overlapping
sub-problems. If a problem can be solved by combining optimal solutions to
non-overlapping sub-problems, the strategy is called "divide and conquer"
instead. This is why merge sort and quick sort are not classified as dynamic
programming problems.

Optimal substructure means that the solution to a given optimization problem
can be obtained by the combination of optimal solutions to its sub-problems.
Such optimal substructures are usually described by means of recursion. For
example, given a graph G=(V,E), the shortest path p from a vertex u to a vertex
v exhibits optimal substructure: take any intermediate vertex w on this
shortest path p. If p is truly the shortest path, then it can be split into
sub-paths p1 from u to w and p2 from w to v such that these, in turn, are
indeed the shortest paths between the corresponding vertices 

Overlapping sub-problems means that the space of sub-problems must be small,
that is, any recursive algorithm solving the problem should solve the same
sub-problems over and over, rather than generating new sub-problems. For
example, consider the recursive formulation for generating the Fibonacci
series: Fi = Fi−1 + Fi−2, with base case F1 = F2 = 1. Then F43 = F42 + F41, and
F42 = F41 + F40. Now F41 is being solved in the recursive sub-trees of both F43
as well as F42. Even though the total number of sub-problems is actually small
(only 43 of them), we end up solving the same problems over and over if we
adopt a naive recursive solution such as this. Dynamic programming takes
account of this fact and solves each sub-problem only once.

The dynamic programming algorithm is based on a recursive function that goes
through all possibilities how to solve, like a brute force algorithm.
However, the dynamic programming algorithm is efficient because it uses
memoization and calculates the answer to each subproblem only once.

memoization = caching an overlapping subproblem's solution.

